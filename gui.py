import sys
from PyQt5.QtWidgets import (
    QApplication, QWidget, QPushButton, QVBoxLayout,
    QFileDialog, QLabel, QMessageBox
)
from PyQt5.QtCore import Qt
from PyQt5.QtCore import QThread, pyqtSignal

from code import inference as inf
from code import models as mdls
#### #################################################
import os
import torch
import librosa
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
#### #################################################
import sounddevice as sd
import time
import pyaudio
import librosa.display
import matplotlib.pyplot as plt

class RecognizeThread(QThread):
    result_signal = pyqtSignal(str)

    def __init__(self, model, sample_rate, record_fn, predict_fn):
        super().__init__()
        self.model = model
        self.sample_rate = sample_rate
        self.record_fn = record_fn
        self.predict_fn = predict_fn

    def run(self):
        buf_len = 10
        clip = self.record_fn()
        length = len(clip)
        buffer = np.zeros(length *buf_len)
        for i in range(1, buf_len):
            clip = self.record_fn()
            print(f"{i} clip stored...")
            buffer[i * length: length * (i+1)] = clip[:, 0]  # å–å·¦è²é“
        for i in range(15):
            emotion, valence, arousal = self.predict_fn(buffer, self.model)
            msg = f"é æ¸¬æƒ…ç·’: {emotion}\nValence: {valence:.2f}, Arousal: {arousal:.2f}"
            self.result_signal.emit(msg)
            clip = self.record_fn()
            buffer[0:-len(clip)-1] = buffer[len(clip):-1]
            buffer[-len(clip)-1:-1] = clip[:, 0]
            time.sleep(1)
        msg = f"é æ¸¬æƒ…ç·’: {emotion}\nValence: {valence:.2f}, Arousal: {arousal:.2f}\nEND"
        self.result_signal.emit(msg)
class EmotionRecognizerGUI(QWidget):
    MODEL_PATH = './pth/emotion_crnn_model.pth'
    SAMPLE_RATE = inf.SAMPLE_RATE
    MAX_DURATION = inf.MAX_DURATION
    MAX_LEN = SAMPLE_RATE * MAX_DURATION
    N_MELS = inf.N_MELS
    device = inf.device
    model = mdls.EmotionCRNN().to(device)  # åŠ ä¸Š .to(device)

    def __init__(self):
        super().__init__()
        self.setWindowTitle("éŸ³æ¨‚æƒ…ç·’è¾¨è­˜")
        self.setGeometry(100, 100, 300, 200)

        self.audio_file_path = None
        self.audio = None
        self.init_ui()

    def init_ui(self):
        layout = QVBoxLayout()

        self.label = QLabel("è«‹é¸æ“‡éŸ³æ¨‚æˆ–éŒ„éŸ³æª”æ¡ˆ", self)
        self.label.setAlignment(Qt.AlignCenter)
        layout.addWidget(self.label)

        self.record_button = QPushButton("å³èˆˆè¾¨è­˜", self)
        self.record_button.clicked.connect(self.realtime_recognize)
        layout.addWidget(self.record_button)

        self.open_file_button = QPushButton("é–‹å•ŸéŸ³æ¨‚æª”æ¡ˆ", self)
        self.open_file_button.clicked.connect(self.open_audio_file)
        layout.addWidget(self.open_file_button)

        self.recognize_button = QPushButton("è¾¨è­˜", self)
        self.recognize_button.clicked.connect(self.recognize_emotion)
        layout.addWidget(self.recognize_button)

        # æ·»åŠ  Discord Bot æ§åˆ¶æŒ‰éˆ•
        self.discord_bot_button = QPushButton("å•Ÿå‹• Discord Bot", self)
        self.discord_bot_button.clicked.connect(self.toggle_discord_bot)
        layout.addWidget(self.discord_bot_button)

        self.discord_play_button = QPushButton("åœ¨ Discord æ’­æ”¾", self)
        self.discord_play_button.clicked.connect(self.play_in_discord)
        self.discord_play_button.setEnabled(False)  # åˆå§‹æ™‚ç¦ç”¨
        layout.addWidget(self.discord_play_button)

        self.setLayout(layout)

    def toggle_discord_bot(self):
        try:
            from bot import start_bot, stop_bot
            
            if self.discord_bot_button.text() == "å•Ÿå‹• Discord Bot":
                # å•Ÿå‹• bot
                TOKEN = ''  # è«‹æ›¿æ›æˆæ‚¨çš„ Discord Bot Token
                if start_bot(TOKEN):
                    self.discord_bot_button.setText("é—œé–‰ Discord Bot")
                    self.discord_play_button.setEnabled(True)
                    QMessageBox.information(self, "æˆåŠŸ", "Discord Bot å·²å•Ÿå‹•")
                else:
                    QMessageBox.warning(self, "è­¦å‘Š", "Discord Bot å·²åœ¨é‹è¡Œä¸­")
            else:
                # é—œé–‰ bot
                if stop_bot():
                    self.discord_bot_button.setText("å•Ÿå‹• Discord Bot")
                    self.discord_play_button.setEnabled(False)
                    QMessageBox.information(self, "æˆåŠŸ", "Discord Bot å·²é—œé–‰")
                else:
                    QMessageBox.warning(self, "è­¦å‘Š", "Discord Bot æœªåœ¨é‹è¡Œ")
        except ImportError as e:
            QMessageBox.critical(self, "éŒ¯èª¤", f"ç„¡æ³•è¼‰å…¥ Discord Bot æ¨¡çµ„: {str(e)}")
        except Exception as e:
            QMessageBox.critical(self, "éŒ¯èª¤", f"æ“ä½œå¤±æ•—: {str(e)}")

    def load_model(self, model_path):
        self.model.load_state_dict(torch.load(model_path, map_location=self.device))  # æ”¯æ´ CPU/GPU éƒ½å¯è¼‰å…¥
        self.model.eval()
        return self.model

    def record_clip(self):
        audio = sd.rec(int(3 * self.SAMPLE_RATE), samplerate=self.SAMPLE_RATE, channels=2, dtype='float32')
        sd.wait()
        return audio
    def record_clip_pyaudio(self, duration=3, sample_rate=16000, channels=1):
        CHUNK = 1024
        FORMAT = pyaudio.paFloat32
        p = pyaudio.PyAudio()

        stream = p.open(format=FORMAT,
                        channels=channels,
                        rate=sample_rate,
                        input=True,
                        frames_per_buffer=CHUNK)

        print("ğŸ™ï¸ Recording...")
        frames = []
        for _ in range(0, int(sample_rate / CHUNK * duration)):
            data = stream.read(CHUNK)
            frames.append(np.frombuffer(data, dtype=np.float32))
        print("ğŸ›‘ Done.")

        stream.stop_stream()
        stream.close()
        p.terminate()

        audio = np.hstack(frames)
        if channels == 2:
            audio = audio.reshape(-1, 2)
        else:
            audio = audio.reshape(-1, 1)

        return audio

    def realtime_recognize(self):
        self.label.setText("ğŸ™ï¸ å³èˆˆè¾¨è­˜ä¸­...")
        model = self.load_model(self.MODEL_PATH)
        self.rec_thread = RecognizeThread(
            model=model,
            sample_rate=self.SAMPLE_RATE,
            #record_fn=self.record_clip,
            record_fn = lambda: self.record_clip_pyaudio(duration=3, sample_rate=self.SAMPLE_RATE, channels=2),
            predict_fn=self.predict_emotion
        )
        self.rec_thread.result_signal.connect(self.update_label)
        self.rec_thread.start()
            
    def update_label(self, text):
        self.label.setText(text)
        
    def open_audio_file(self):
        file_name, _ = QFileDialog.getOpenFileName(
            self, "é¸æ“‡éŸ³æ¨‚æª”æ¡ˆ", "", "éŸ³è¨Šæª”æ¡ˆ (*.wav *.mp3 *.flac)"
        )
        if file_name:
            self.audio_file_path = file_name
            self.label.setText(f"å·²é¸æ“‡ï¼š{file_name.split('/')[-1]}")
            self.audio, sr = librosa.load(self.audio_file_path, sr=self.SAMPLE_RATE)
            self.plot_MFCC(self.audio, sr)
    def recognize_emotion(self):
        if not self.audio_file_path:
            QMessageBox.warning(self, "éŒ¯èª¤", "è«‹å…ˆé–‹å•ŸéŸ³æ¨‚æª”æ¡ˆæˆ–éŒ„éŸ³")
            return

        # TODO: åŠ å…¥éŸ³æ¨‚æƒ…ç·’è¾¨è­˜çš„ç¨‹å¼é‚è¼¯
        # self.audio_file_path æ˜¯é¸æ“‡çš„éŸ³è¨Šæª”æ¡ˆè·¯å¾‘
        self.label.setText("è¾¨è­˜ä¸­")
        
        model = self.load_model(self.MODEL_PATH)
        emotion, valence, arousal = self.predict_emotion(self.audio, model)
        msg = f"The predicted emotion for the audio file is: {emotion}" + '\n' + \
              f"Valence: {valence}, Arousal: {arousal}"
        self.label.setText("è¾¨è­˜å®Œæˆ")
        QMessageBox.information(self, "è¾¨è­˜å®Œæˆ", msg)
    
    def predict_emotion(self, audio, model):
        # è¼‰å…¥éŸ³æ¨‚æª”æ¡ˆ
        y, sr = audio, self.SAMPLE_RATE
        y = librosa.util.fix_length(y, size=self.MAX_LEN)
        mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=self.N_MELS)
        mel_db = librosa.power_to_db(mel, ref=np.max)
        mel_db = torch.tensor(mel_db, dtype=torch.float32)
        
        mel_db = (mel_db - mel_db.mean()) / mel_db.std()
        mel_db = mel_db.unsqueeze(0).unsqueeze(0).to(self.device)  # [1, 1, N_MELS, T]

        # é æ¸¬
        with torch.no_grad():
            pred = model(mel_db)
            # valence, arousal = pred[0].numpy()
            valence, arousal = pred[0].cpu().numpy()
            emotion_label = inf.get_emotion_label(valence, arousal)
        
        return emotion_label, valence, arousal

    def plot_MFCC(self, audio, sr):
        mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)  # é€šå¸¸å–å‰13ç¶­

        # ç¹ªåœ–
        plt.figure(figsize=(10, 4))
        librosa.display.specshow(mfcc, x_axis='time')
        plt.colorbar()
        plt.title('MFCC')
        plt.tight_layout()
        plt.show()

    def play_in_discord(self):
        if not self.audio_file_path:
            QMessageBox.warning(self, "éŒ¯èª¤", "è«‹å…ˆé–‹å•ŸéŸ³æ¨‚æª”æ¡ˆ")
            return
            
        try:
            # è¤‡è£½éŸ³æ¨‚æª”æ¡ˆåˆ° Discord bot çš„éŸ³æ¨‚ç›®éŒ„
            import shutil
            import os
            import sys
            
            # ç²å–ç•¶å‰æ–‡ä»¶çš„ç›®éŒ„
            current_dir = os.path.dirname(os.path.abspath(__file__))
            
            # ç¢ºä¿ music ç›®éŒ„å­˜åœ¨
            music_dir = os.path.join(current_dir, 'music')
            if not os.path.exists(music_dir):
                os.makedirs(music_dir)
                
            # è¤‡è£½æª”æ¡ˆ
            file_name = os.path.basename(self.audio_file_path)
            target_path = os.path.join(music_dir, file_name)
            
            # æª¢æŸ¥æºæ–‡ä»¶å’Œç›®æ¨™æ–‡ä»¶æ˜¯å¦ç›¸åŒ
            if os.path.normpath(self.audio_file_path) != os.path.normpath(target_path):
                shutil.copy2(self.audio_file_path, target_path)
            
            # è¨­å®šè‡ªå‹•æ§åˆ¶åƒæ•¸
            try:
                from bot import setup_auto_control
                # ä½¿ç”¨æ‚¨çš„ Discord é »é“ ID
                channel_id = 851380042117283860  # è«‹æ›¿æ›æˆæ‚¨çš„é »é“ID
                setup_auto_control(channel_id, file_name)
                
                # ç­‰å¾…ä¸€ä¸‹ç¢ºä¿è¨­ç½®ç”Ÿæ•ˆ
                import time
                time.sleep(2)
                
                QMessageBox.information(self, "æˆåŠŸ", f"å·²è¨­å®šåœ¨ Discord æ’­æ”¾ {file_name}\nè«‹æª¢æŸ¥ Discord é »é“æ˜¯å¦æœ‰éŸ³æ¨‚æ’­æ”¾")
            except ImportError as e:
                QMessageBox.critical(self, "éŒ¯èª¤", f"ç„¡æ³•è¼‰å…¥ Discord Bot æ¨¡çµ„: {str(e)}\nè«‹ç¢ºä¿å·²å®‰è£ discord.py ä¸¦è¨­ç½®äº† Bot Token")
            except Exception as e:
                QMessageBox.critical(self, "éŒ¯èª¤", f"è¨­å®š Discord Bot å¤±æ•—: {str(e)}")
            
        except Exception as e:
            QMessageBox.critical(self, "éŒ¯èª¤", f"æ’­æ”¾è¨­å®šå¤±æ•—: {str(e)}")

if __name__ == "__main__":
    app = QApplication(sys.argv)
    window = EmotionRecognizerGUI()
    window.show()
    sys.exit(app.exec_())
